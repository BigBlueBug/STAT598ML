{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the code is written on my own after understanding the background, the logical structure (and some function names) of the entire implementation follows implementations found on the internet, especially msurtsukov's implementation on the Github repo \"neural-ode\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# lets check if cuda is available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(use_cuda)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use any ODE solver we wish, but let us try the simple Euler's method first. Euler's method is good enough for simulating our spiral (archimedean spiral), but\n",
    "when trying to simulate other dynamical systems we will use a better method like RK4 (Runge-Kutta) or Dopri5!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ODE initial value problem solver - Euler's method \n",
    "def ode_solve(z0,t0,t1,f):\n",
    "    h_max = 0.05\n",
    "    n_steps = int(np.ceil(abs(t1 - t0) / h_max).max())\n",
    "    h = (t1 - t0) / n_steps\n",
    "    t = t0\n",
    "    z = z0\n",
    "\n",
    "    for i_step in range(n_steps):\n",
    "        z = z + h * f(z, t)\n",
    "        t = t + h\n",
    "\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3584859224085422"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example using Euler ODE solver\n",
    "def f(z,t):\n",
    "    return -z\n",
    "ode_solve(1,0,1,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class of parameterized dynamics\n",
    "##Adjoints calculated with PyTorch\n",
    "class ODEF(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.t = None\n",
    "    \n",
    "    def forward(self, z):\n",
    "        raise NotImplementedError('`forward` method must be implemented in subclass.')\n",
    "        \n",
    "    def flatten_parameters(self):\n",
    "        return torch.cat([p.flatten() for p in self.parameters()])\n",
    "    \n",
    "    def forward_with_grad(self, z, t, grad_outputs):\n",
    "        batch_size = z.shape[0]\n",
    "        \n",
    "        if self.t is None or not torch.allclose(t, self.t):\n",
    "            self.t = t\n",
    "            self.out = self.forward(z)\n",
    "        \n",
    "        a = grad_outputs\n",
    "        adfdz, adfdt, *adfdp = torch.autograd.grad(\n",
    "            (self.out,), (z,) + tuple(self.parameters()) + (t,),\n",
    "            grad_outputs=(a), allow_unused=True, retain_graph=True\n",
    "        )\n",
    "        \n",
    "        if adfdp is not None:\n",
    "            adfdp = torch.cat([p_grad.flatten() for p_grad in adfdp]).unsqueeze(0)\n",
    "            adfdp = adfdp.expand(batch_size, -1) / batch_size\n",
    "        if adfdt is not None:\n",
    "            adfdt = adfdt.expand(batch_size, 1) / batch_size\n",
    "        \n",
    "        return self.out, adfdz, adfdt, adfdp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##main adjoint method\n",
    "#This adjoint method is almost the same as the one used by MSurtsukov, which inturn is pretty similar to the original authors implementation in torchdiffeq \n",
    "#In the original code, ODEadjoint implementation is written in cpp, which is supposedly more efficient. \n",
    "\n",
    "class ODEAdjoint(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, z0, t, flat_parameters, func):  ##forward pass\n",
    "        assert isinstance(func, ODEF)\n",
    "        bs, *z_shape = z0.shape\n",
    "        time_len = t.shape[0]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            z = torch.zeros((time_len, bs, *z_shape), device=z0.device, dtype=z0.dtype)\n",
    "            z[0] = z0\n",
    "            for i in range(time_len - 1):\n",
    "                z0 = ode_solve(z0, t[i], t[i+1], func)\n",
    "                z[i+1] = z0\n",
    "\n",
    "        ctx.func = func\n",
    "        ctx.save_for_backward(t, z.clone(), flat_parameters)\n",
    "        return z\n",
    "\n",
    "    @staticmethod  #make these methods static\n",
    "    def backward(ctx, dLdz):    ##backward pass\n",
    "        \"\"\"\n",
    "        dLdz shape: time_len, batch_size, *z_shape\n",
    "        \"\"\"\n",
    "        func = ctx.func\n",
    "        t, z, flat_parameters = ctx.saved_tensors\n",
    "        time_len, bs, *z_shape = z.shape\n",
    "        n_dim = np.prod(z_shape)\n",
    "        n_params = flat_parameters.shape[0]\n",
    "\n",
    "        # Dynamics of Aug system which are to be calculated backwards in time\n",
    "        def augmented_dynamics(aug_z_i, t_i):\n",
    "            z_i, a = aug_z_i[:, :n_dim], aug_z_i[:, n_dim:2*n_dim]  \n",
    "\n",
    "            # Unflatten z and a\n",
    "            z_i = z_i.view(bs, *z_shape)\n",
    "            a = a.view(bs, *z_shape)\n",
    "            with torch.set_grad_enabled(True):\n",
    "                t_i = t_i.detach().requires_grad_(True)\n",
    "                z_i = z_i.detach().requires_grad_(True)\n",
    "                func_eval, adfdz, adfdt, adfdp = func.forward_with_grad(z_i, t_i, grad_outputs=a)  # bs, *z_shape\n",
    "                adfdz = adfdz.to(z_i) if adfdz is not None else torch.zeros((bs, *z_shape), device=z_i.device, dtype=z_i.dtype)\n",
    "                adfdp = adfdp.to(z_i) if adfdp is not None else torch.zeros((bs, n_params), device=z_i.device, dtype=z_i.dtype)\n",
    "                adfdt = adfdt.to(z_i) if adfdt is not None else torch.zeros((bs, 1), device=z_i.device, dtype=z_i.dtype)\n",
    "\n",
    "            # Flatten f and adfdz\n",
    "            func_eval = func_eval.view(bs, n_dim)\n",
    "            adfdz = adfdz.view(bs, n_dim) \n",
    "            return torch.cat((func_eval, -adfdz, -adfdp, -adfdt), dim=1)\n",
    "\n",
    "        dLdz = dLdz.view(time_len, bs, n_dim)  # flatten dLdz for convenience\n",
    "        with torch.no_grad():\n",
    "            ## Create placeholders for output gradients\n",
    "            # Prev computed backwards adjoints to be adjusted by direct gradients\n",
    "            adj_z = torch.zeros((bs, n_dim), device=dLdz.device, dtype=dLdz.dtype)\n",
    "            adj_p = torch.zeros((bs, n_params), device=dLdz.device, dtype=dLdz.dtype)\n",
    "            # In contrast to z and p we need to return gradients for all times\n",
    "            adj_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralODE:\n",
    "    def __init__(self, func):\n",
    "        assert isinstance(func, ODEF)\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, z0, t=np.array([0., 1.]), return_whole_sequence=False):\n",
    "        z0 = z0.astype(np.float32)\n",
    "        t = t.astype(np.float32)\n",
    "        z = ODEAdjoint.apply(torch.from_numpy(z0), torch.from_numpy(t), self.func.flatten_parameters(), self.func).numpy()\n",
    "        if return_whole_sequence:\n",
    "            return z\n",
    "        else:\n",
    "            return z[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearODEF(ODEF):\n",
    "    def __init__(self, W):\n",
    "        super(LinearODEF, self).__init__()\n",
    "        self.W = W\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        return np.matmul(x, self.W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpiralFunctionExample(LinearODEF):\n",
    "    def __init__(self):\n",
    "        super(SpiralFunctionExample, self).__init__(Tensor([[-0.1, -1.], [1., -0.1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomLinearODEF(LinearODEF):\n",
    "    def __init__(self):\n",
    "        super(RandomLinearODEF, self).__init__(torch.randn(2, 2)/2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestODEF(ODEF):\n",
    "    def __init__(self, A, B, x0):\n",
    "        super(TestODEF, self).__init__()\n",
    "        self.A = nn.Linear(2, 2, bias=False)\n",
    "        self.A.weight = nn.Parameter(A)\n",
    "        self.B = nn.Linear(2, 2, bias=False)\n",
    "        self.B.weight = nn.Parameter(B)\n",
    "        self.x0 = nn.Parameter(x0)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        xTx0 = torch.sum(x*self.x0, dim=1)\n",
    "        sigmoid_xTx0 = torch.sigmoid(xTx0)\n",
    "        A_diff = sigmoid_xTx0 * self.A(x - self.x0)\n",
    "        B_diff = sigmoid_xTx0 * self.B(x + self.x0)\n",
    "        dxdt = A_diff + B_diff\n",
    "        return dxdt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNODEF(ODEF):\n",
    "    def __init__(self, in_dim, hid_dim, time_invariant=False):\n",
    "        super().__init__()\n",
    "        self.time_invariant = time_invariant\n",
    "\n",
    "        if time_invariant is True:\n",
    "            self.lin1 = nn.Linear(in_dim, hid_dim)\n",
    "        else:\n",
    "            self.lin1 = nn.Linear(in_dim+1, hid_dim)\n",
    "        self.lin2 = nn.Linear(hid_dim, hid_dim)\n",
    "        self.lin3 = nn.Linear(hid_dim, in_dim)\n",
    "        self.elu = nn.ELU(inplace=True)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        if not self.time_invariant:\n",
    "            x = torch.cat((x, t), dim=-1)\n",
    "\n",
    "        h = self.elu(self.lin1(x))\n",
    "        h = self.elu(self.lin2(h))\n",
    "        out = self.lin3(h)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_np(x):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        x = x.detach().cpu().numpy()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trajectories(obs=None, times=None, trajs=None, save=None, figsize=(16, 8)):\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    if obs is not None:\n",
    "        if times is None:\n",
    "            times = [None] * len(obs)\n",
    "        for o, t in zip(obs, times):\n",
    "            o, t = to_np(o), to_np(t)\n",
    "            for b_i in range(o.shape[1]):\n",
    "                ax.scatter(o[:, b_i, 0], o[:, b_i, 1], c=t[:, b_i, 0], cmap=cm.plasma)\n",
    "\n",
    "    if trajs is not None: \n",
    "        for z in trajs:\n",
    "            z = to_np(z)\n",
    "            ax.plot(z[:, 0, 0], z[:, 0, 1], lw=1.5)\n",
    "        if save is not None:\n",
    "            plt.savefig(save)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conduct_experiment(ode_true, ode_trained, n_steps, name, plot_freq=10):\n",
    "    # Create data\n",
    "    z0 = torch.tensor([[0.6, 0.3]], requires_grad=True)\n",
    "\n",
    "    t_max = 6.29*5\n",
    "    n_points = 200\n",
    "\n",
    "    index_np = np.arange(0, n_points, 1, dtype=np.int)\n",
    "    index_np = np.hstack([index_np[:, None]])\n",
    "    times_np = np.linspace(0, t_max, num=n_points)\n",
    "    times_np = np.hstack([times_np[:, None]])\n",
    "\n",
    "    times = torch.from_numpy(times_np[:, :, None]).to(z0)\n",
    "    obs = ode_true(z0, times, return_whole_sequence=True).detach()\n",
    "    obs = obs + torch.randn_like(obs) * 0.01\n",
    "\n",
    "    # Get trajectory of random timespan\n",
    "    min_delta_time = 1.0\n",
    "    max_delta_time = 5.0\n",
    "    max_points_num = 32\n",
    "\n",
    "    def create_batch():\n",
    "        t0 = np.random.uniform(0, t_max - max_delta_time)\n",
    "        t1 = t0 + np.random.uniform(min_delta_time, max_delta_time)\n",
    "\n",
    "        idx = sorted(np.random.permutation(index_np[(times_np > t0) & (times_np < t1)])[:max_points_num])\n",
    "\n",
    "        obs_ = obs[idx]\n",
    "        ts_ = times[idx]\n",
    "        return obs_, ts_\n",
    "\n",
    "    # Train Neural ODE\n",
    "    optimizer = torch.optim.Adam(ode_trained.parameters(), lr=0.01)\n",
    "    for i in range(n_steps):\n",
    "        obs_, ts_ = create_batch()\n",
    "\n",
    "        z_ = ode_trained(obs_[0], ts_, return_whole_sequence=True)\n",
    "        loss = F.mse_loss(z_, obs_.detach())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % plot_freq == 0:\n",
    "            z_p = ode_trained(z0, times, return_whole_sequence=True)\n",
    "\n",
    "            obs_np = to_np(obs)\n",
    "            times_np = to_np(times)\n",
    "            z_p_np = to_np(z_p)\n",
    "            plot_trajectories(obs=[obs_np], times=[times_np], trajs=[z_p_np], save=f\"/home/rmogalap/FP/{name}/{i}.png\")\n",
    "            clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conduct_experiment(ode_true, ode_trained, 900, \"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##other spiral graph\n",
    "func = TestODEF(Tensor([[-0.1, -0.5], [0.5, -0.1]]), Tensor([[0.2, 1.], [-1, 0.2]]), Tensor([[-1., 0.]]))\n",
    "ode_true = NeuralODE(func)\n",
    "\n",
    "func = NNODEF(2, 16, time_invariant=True)\n",
    "ode_trained = NeuralODE(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conduct_experiment(ode_true, ode_trained, 3000, \"comp\", plot_freq=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
